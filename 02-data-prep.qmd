# Data Preparation {#sec-data}

---

> **Chapter Context.** Before any statistical analysis can proceed, the raw dataset must be transformed into a form that is both analytically tractable and physically credible. This chapter documents three decisions that have downstream consequences throughout the report: the syntactic and temporal standardisation of the raw data, the design of a three-stage imputation pipeline for the substantial missingness in key meteorological variables, and the detection and conservative handling of extended sensor failures. Each decision is motivated by the properties of the data-generating process rather than by computational convenience.

---

## Computational Environment

```{r}
#| label: setup-libraries-data
#| echo: true
#| message: false
#| warning: false
#| results: 'hide'

librarian::shelf(
  tidyverse,
  tidymodels,
  kableExtra,
  patchwork,
  skimr,
  gridExtra,
  gtsummary,
  janitor,
  corrplot,
  sjPlot,
  scales,
  GGally,
  car,
  forcats,
  performance,
  glmmTMB,
  splines,
  mgcv,
  DHARMa,
  zoo,
  ggpubr,
  ggridges,
  caret,
  rstatix,
  Metrics,
  mice,
  missRanger,
  ranger,
  cocor,
  multcompView,
  lmtest,
  aod,
  pROC
)

df <- read_csv("data/weatherAUS.csv")
source(here::here("utils.R"))
```

The package selection reflects the specific demands of meteorological data analysis. The core wrangling and visualisation toolkit (`tidyverse`, `janitor`, `kableExtra`) handles the routine data manipulation. The modelling stack (`glmmTMB`, `DHARMa`, `splines`, `mgcv`) is assembled for the zero-inflated mixed-effects framework documented in later chapters. The imputation stack (`missRanger`, `mice`, `ranger`) supports the Random Forest and chained imputation procedures described below. Libraries are loaded via `librarian`, which installs any missing packages automatically and ensures reproducibility across environments.

---

## Initial Cleaning and Standardisation

The raw dataset undergoes three transformations before imputation.

```{r}
#| label: data-cleaning
#| echo: true
#| message: false
#| warning: false

df_clean <- df %>%
  clean_names() %>%
  mutate(
    date = as.Date(date),
    month = as.factor(month(date)),
    day = as.factor(wday(date, label = TRUE))
  ) %>%
  filter(!is.na(rainfall))

df_clean %>%
  head() %>%
  kable(caption = "Head of Cleaned Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

df_clean %>%
  tail() %>%
  kable(caption = "Tail of Cleaned Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

**Column standardisation.** All variable names are converted to `snake_case` via `janitor::clean_names()`. This is a low-cost operation that eliminates downstream ambiguity in variable references and ensures that the codebase is internally consistent across all chapters.

**Temporal feature extraction.** The `date` column is typed as a `Date` object and two derived features are extracted immediately: `month` (as a factor) and `day` (day of week, labelled). Both are used in the EDA (@sec-eda) and feature engineering(@sec-feature-eng) chapters. Month captures the seasonal structure of Australian precipitation; day of week, while not expected to carry physical signal, is retained as a potential control variable for reporting artefacts in the raw data.

**Target filtering.** Observations where `rainfall` is missing are removed. Because the primary modelling objective is to predict rainfall occurrence and intensity, records without a ground-truth rainfall measurement have no supervised learning value and cannot contribute to either the hurdle or the intensity component of the ZIG model.

---

## Hybrid Imputation Strategy

The EDA chapter documents that `sunshine` is missing in 47.7% of records and `evaporation` in 42.5%, while cloud cover variables are missing in 37 to 40% of records. Standard listwise deletion, which removes any row containing a missing value, would eliminate nearly half the dataset and introduce a systematic geographic bias toward well-instrumented stations. Standard mean or median imputation would fill the gaps but would shrink the marginal variance of the imputed variables and distort their correlations with other predictors.

The imputation pipeline described here addresses these problems through a three-stage strategy, each stage targeting a different structural type of missingness.

### Stage 1: Missingness Flags

Before any imputation takes place, binary indicator variables are created for the four variables with high missingness: `sunshine_imp_flagged`, `evap_imp_flagged`, `cloud3pm_imp_flagged`, and `cloud9am_imp_flagged`. A flag value of 1 indicates that the corresponding observation was subsequently imputed.

The motivation is that the pattern of missingness is not random with respect to atmospheric conditions. Sunshine sensors may fail more frequently during prolonged cloud cover or heavy rain, and evaporation pan measurements may be suspended during equipment maintenance that is itself weather-triggered. If missingness is informative in this way, the imputation flags allow the model to learn this association rather than treating the imputed and observed values as exchangeable.

### Stage 2: Temporal Interpolation

The eight variables with strong day-to-day persistence and smooth trajectories; minimum and maximum temperature, morning and afternoon temperature, morning and afternoon pressure, and morning and afternoon humidity are imputed via linear interpolation grouped by location. The interpolation is bounded by a maximum gap of five consecutive missing days.

The physical rationale is direct. Atmospheric temperature, pressure, and humidity evolve as continuous functions of time: a value of $20^\circ$C on Monday and $22^\circ$C on Wednesday implies a physically reasonable estimate of $21^\circ$C on Tuesday. Filling this gap with a global mean or a seasonal climatology would ignore the temporal context that makes the missing value estimable with high confidence. The five-day cap prevents the interpolation from producing extended sequences of synthetic data over sustained sensor outages, which would introduce artificial smoothness into periods that may have been meteorologically volatile.

### Stage 3: Multivariate Random Forest Imputation

The remaining gaps, concentrated in `sunshine`, `evaporation`, and cloud cover, cannot be addressed by temporal interpolation because these variables do not exhibit simple smooth trajectories. Their behaviour is conditional on specific atmospheric configurations and their relationships to other predictors are non-linear.

A two-pass approach is used. The first pass applies `missRanger`, iterative Random Forest imputation with Predictive Mean Matching across the dataset as a whole. PMM ensures that imputed values are drawn from the observed distribution of each variable rather than extrapolated from the regression surface, which preserves the marginal distributions and prevents the variance shrinkage associated with direct regression imputation.

The second pass applies targeted `mice` imputation using Random Forest as the imputation method, with scientifically motivated predictor sets for each variable. Cloud cover at 9 AM and 3 PM are predicted from humidity and pressure, which are the direct physical precursors of atmospheric stability and cloud formation. Sunshine duration is predicted from cloud cover and temperature, reflecting the inverse relationship between overcast conditions and insolation. Evaporation is predicted from wind speed, temperature, humidity, sunshine, and location, corresponding to the Penman-Monteith physical drivers of evaporative demand.

Using targeted predictors rather than allowing the algorithm to use all available variables serves two purposes: it reduces the risk of imputing on spurious correlations present in the training data but absent from the physical system, and it makes the imputation procedure interpretable and auditable.

### Ghost Sensor Detection

Some stations in the network experience sustained equipment failures in which a specific sensor records no values for an entire season or longer. If these extended gaps are passed to the Random Forest imputer, the algorithm will produce synthetic values for the entire period using correlations with other variables from the same location. This is methodologically unsound: the correlations learned from the observed periods at that station may not hold during the failure period, and producing a long uninterrupted synthetic series could create artificial patterns that mislead downstream analysis.

Ghost sensors are identified as any location-variable pair where the missingness rate exceeds 90%. After the full imputation pipeline runs, values for these pairs are reverted to missing. This conservative treatment accepts that some data is genuinely unrecoverable, while allowing the imputation to work effectively on the shorter gaps that constitute recoverable missingness.

```{r}
#| label: imputation-strategy
#| echo: true
#| eval: false

clean_and_impute_weather <- function(df) {
  MAXGAP <- 5
  GHOST_THRESHOLD <- 0.90
  PMM_K <- 5
  MAXITER <- 4

  df <- df %>%
    clean_names() %>%
    mutate(
      date = as.Date(date),
      month = as.factor(month(date)),
      day = as.factor(wday(date, label = TRUE)),
      day_of_year = yday(date)
    ) %>%
    filter(!is.na(rainfall)) %>%
    select(-rain_tomorrow)

  # Missingness flags
  df_flagged <- df %>%
    mutate(
      sunshine_imp_flagged = ifelse(is.na(sunshine), 1, 0),
      evap_imp_flagged = ifelse(is.na(evaporation), 1, 0),
      cloud3pm_imp_flagged = ifelse(is.na(cloud3pm), 1, 0),
      cloud9am_imp_flagged = ifelse(is.na(cloud9am), 1, 0)
    )

  # Temporal interpolation for smooth continuous variables
  interp_vars <- c(
    "min_temp",
    "max_temp",
    "temp9am",
    "temp3pm",
    "pressure9am",
    "pressure3pm",
    "humidity9am",
    "humidity3pm"
  )

  df_interp <- df_flagged %>%
    group_by(location) %>%
    arrange(date, .by_group = TRUE) %>%
    mutate(across(
      all_of(interp_vars),
      ~ na.approx(., maxgap = MAXGAP, na.rm = FALSE, rule = 2)
    )) %>%
    ungroup()

  # Ghost sensor identification
  ghost_prone_vars <- c("sunshine", "evaporation", "cloud3pm", "cloud9am")

  ghost_pairs <- df_interp %>%
    select(location, all_of(ghost_prone_vars)) %>%
    pivot_longer(
      cols = all_of(ghost_prone_vars),
      names_to = "variable",
      values_to = "value"
    ) %>%
    group_by(location, variable) %>%
    summarise(
      miss_rate = mean(is.na(value)) * 100,
      .groups = "drop"
    ) %>%
    filter(miss_rate > (GHOST_THRESHOLD * 100)) %>%
    select(location, variable)

  cat(sprintf("  Found %d ghost sensor instances\n\n", nrow(ghost_pairs)))

  # missRanger first pass
  imputation_data <- df_interp %>%
    mutate(
      sin_month = sin(2 * pi * as.numeric(month) / 12),
      cos_month = cos(2 * pi * as.numeric(month) / 12),
      sin_doy = sin(2 * pi * day_of_year / 365),
      cos_doy = cos(2 * pi * day_of_year / 365)
    )

  metadata_cols <- imputation_data %>% select(date)
  imputation_cols <- imputation_data %>% select(-date)

  imputed_data <- missRanger(
    data = imputation_cols,
    pmm.k = PMM_K,
    num.trees = 100,
    sample.fraction = 0.3,
    min.node.size = 10,
    seed = 123,
    verbose = 1,
    maxiter = MAXITER
  )

  df_imputed <- bind_cols(metadata_cols, imputed_data) %>%
    select(-starts_with("sin_"), -starts_with("cos_"))

  # Ghost sensor sanitisation
  if (nrow(ghost_pairs) > 0) {
    ghost_map <- split(ghost_pairs$location, ghost_pairs$variable)
    df_imputed <- df_imputed %>%
      mutate(across(
        names(ghost_map),
        ~ replace(., location %in% ghost_map[[cur_column()]], NA)
      ))
    cat(sprintf("  Reverted %d instances to NA\n\n", nrow(ghost_pairs)))
  }

  # Targeted MICE with Random Forest
  init <- mice(df_imputed, maxit = 0)
  pred <- init$predictorMatrix
  meth <- init$method
  pred[,] <- 0

  meth["sunshine"] <- "rf"
  meth["evaporation"] <- "rf"
  meth["cloud9am"] <- "rf"
  meth["cloud3pm"] <- "rf"

  sun_predictors <- intersect(
    colnames(df_imputed),
    c("cloud9am", "cloud3pm", "max_temp", "humidity3pm", "location", "month")
  )
  evap_predictors <- intersect(
    colnames(df_imputed),
    c(
      "wind_gust_speed",
      "max_temp",
      "humidity3pm",
      "sunshine",
      "location",
      "month"
    )
  )
  cloud_predictors <- intersect(
    colnames(df_imputed),
    c("humidity9am", "humidity3pm", "pressure9am", "location", "month")
  )

  if ("sunshine" %in% rownames(pred)) {
    pred["sunshine", sun_predictors] <- 1
  }
  if ("evaporation" %in% rownames(pred)) {
    pred["evaporation", evap_predictors] <- 1
  }
  if ("cloud9am" %in% rownames(pred)) {
    pred["cloud9am", cloud_predictors] <- 1
  }
  if ("cloud3pm" %in% rownames(pred)) {
    pred["cloud3pm", cloud_predictors] <- 1
  }

  ignore_cols <- grep("_flagged$|^date$", colnames(pred), value = TRUE)
  pred[, ignore_cols] <- 0

  imp <- mice(
    df_imputed,
    method = meth,
    predictorMatrix = pred,
    m = 1,
    maxit = 5,
    seed = 123,
    printFlag = FALSE
  )

  df_final <- complete(imp)
  return(df_final)
}
```

```{r}
#| label: clean-and-save-imputed
#| message: false
#| warning: false
#| eval: false

df_final <- clean_and_impute_weather(df)
write_csv(df_final, "data/df_final.csv")
```

```{r}
#| label: load-imputed-data
#| echo: false
#| message: false
#| warning: false

df_final <- read_csv("data/df_final.csv")
```

---

## Post-Imputation Dataset Properties

The three-stage pipeline produces a dataset with the following properties, which collectively justify the methodological investment relative to simpler alternatives.

**Retention.** No observations are discarded on the basis of partial missingness. The imputation flags preserve the distinction between observed and imputed values, allowing downstream models to treat the two categories differently if the flag coefficients prove significant.

**Temporal coherence.** The interpolated variables maintain their within-location autocorrelation structure. The five-day cap prevents the interpolation from extending across genuine data voids.

**Distributional fidelity.** Predictive Mean Matching in Stage 3 ensures that imputed values for `sunshine`, `evaporation`, and cloud cover are drawn from the observed empirical distribution of each variable rather than from the regression surface. The marginal distributions and cross-variable correlations of these features are preserved, which is critical for the interaction terms and derived meteorological indices constructed in the feature engineering chapter.

**Conservative treatment of extended failures.** Observations from ghost sensors remain missing rather than being synthetically reconstructed. This is a deliberate acceptance of reduced sample size in exchange for data integrity: a complete but fabricated series is more damaging to downstream inference than an honest gap.

```{r}
#| label: save-clean-data
#| include: false

if (!dir.exists("data")) {
  dir.create("data")
}
saveRDS(df_clean, "data/df_clean.rds")
```