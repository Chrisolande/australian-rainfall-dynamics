---
title: "In-Depth Analysis of Australian Weather Data: Zero-Inflated Models"
author: "Chris Olande"
date: "2026-01-14"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    highlight: tango
    df-print: paged
    fig-width: 10
    fig-height: 6
---

# Introduction

This report presents a comprehensive analysis of the Australian weather dataset (`weatherAUS.csv`), with a specific focus on the statistical challenges posed by **zero-inflated rainfall data** characterized by a high frequency of days with no precipitation. By examining daily observations from diverse locations across Australia, we aim to accurately model both the *occurrence* and *intensity* of rainfall using advanced statistical frameworks.

All computational workflows are implemented in **R**, leveraging the `tidyverse` ecosystem for data manipulation and the `rstatix` and `glmmTMB` packages for sophisticated modeling.

## Objectives

To achieve a robust analysis, this report addresses the following key objectives:

-   **Data Integrity & Imputation:** Execute rigorous data cleaning and implement a Random Forest imputation strategy to handle missing values without discarding critical information.

-   **Exploratory Analysis (EDA):** Quantify the extent of zero-inflation (approximately 64% of observations) and visualize distributional anomalies.

-   **Temporal Dynamics:** Investigate the "persistence" effect to determine how prior weather states influence current rainfall probabilities (Markov Chain analysis).

-   **Model Selection:** Evaluate the performance of standard linear models against **Zero-Inflated Gamma (Hurdle)** models to identify the optimal approach for Australian weather patterns.

## Loading Libraries and Data

We begin by loading the necessary R libraries and reading the dataset.

```{r load-libs-data}
librarian::shelf(
  tidyverse,
  tidymodels,
  kableExtra,
  patchwork,
  skimr,
  gridExtra,
  janitor,
  corrplot,
  scales,
  GGally,
  car,
  forcats,
  performance,
  glmmTMB,
  splines,
  mgcv,
  DHARMa,
  zoo,
  ggpubr,
  ggridges,
  caret,
  rstatix,
  Metrics,
  mice,
  missRanger,
  ranger
)

df <- read_csv("data/weatherAUS.csv")
```

The dataset contains `r nrow(df)` rows and `r ncol(df)` columns, with variables including temperature, humidity, wind, and rainfall data.

## Initial Data Preview

Let's examine the first and last few rows of the dataset to understand the structure.

```{r preview}
df %>%
  head() %>%
  kable(caption = "First 6 rows of the dataset", format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

df %>%
  tail() %>%
  kable(caption = "Last 6 rows of the dataset", format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The data includes dates, locations, and various weather measurements. Note that some columns have missing values, which we'll address later.

# Data Cleaning

## Cleaning Column Names and Adding Derived Variables

We clean the column names using `janitor::clean_names()` and add derived variables for month and day of the week to capture seasonality.

```{r clean-data}
df_clean <- df %>%
  clean_names() %>%
  mutate(
    date = as.Date(date),
    month = as.factor(month(date)),
    day = as.factor(wday(date, label = TRUE))
  ) %>%
  filter(!is.na(rainfall))
```

This results in a cleaned dataset with `r nrow(df_clean)` observations.

## Checking for Duplicates

```{r duplicates}
duplicates <- df_clean %>%
  get_dupes()

print(paste("Number of duplicate rows: ", nrow(duplicates)))
```

No duplicates found, which is good for data integrity.

## Column Names

```{r columns}
df_clean %>%
  names()
```

The dataset now has 25 columns, including the new `month` and `day` variables.

# Missing Values Analysis

## Overall Missing Values

```{r missing-total}
print(paste("Total Missing values: ", sum(is.na(df))))
```

The original dataset has a significant number of missing values (`r sum(is.na(df))`) missing values in total.

## Missing Values by Column

```{r missing-by-col}
missing_tab <- df_clean %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(everything(), names_to = "column", values_to = "pct_missing") %>%
  arrange(desc(pct_missing))

missing_tab %>%
  kable(caption = "Percentage of missing values by column") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Variables like `sunshine` and `evaporation` have high missing rates (\>40%), while others like `rainfall` have none (after filtering). This necessitates robust imputation rather than deletion.

# Summary Statistics for Rainfall

Rainfall is our target variable. Let's examine its distribution.

```{r rainfall-stats}
rainfall_stats <- df_clean %>%
  summarise(
    n = n(),
    mean = mean(rainfall),
    median = median(rainfall),
    sd = sd(rainfall),
    min = min(rainfall),
    max = max(rainfall),
    q25 = quantile(rainfall, 0.25),
    q75 = quantile(rainfall, .75),
    iqr = IQR(rainfall),
    n_zeros = sum(rainfall == 0),
    pct_zeros = mean(rainfall == 0) * 100,
    n_large = sum(rainfall > 100),
    pct_large = mean(rainfall > 100) * 100,
    skewness = moments::skewness(rainfall),
    kurtosis = moments::kurtosis(rainfall)
  )

t(rainfall_stats) %>%
  kable(caption = "Rainfall summary statistics")
```

Key insights: - Mean rainfall: `r round(rainfall_stats$mean, 2)` mm - `r round(rainfall_stats$pct_zeros, 1)`% of days have zero rainfall (zero-inflation) - Highly skewed distribution (skewness: `r round(rainfall_stats$skewness, 2)`)

## Zero-Inflation Check

```{r zero-inflation}
rain_check <- df_clean %>%
  summarise(
    total_days = n(),
    dry_days = sum(rainfall == 0),
    rainy_days = sum(rainfall > 0),
    zero_inflation_pct = (dry_days / total_days) * 100
  )

rain_check %>%
  kable(caption = "Zero-inflation summary") %>%
  kable_styling(bootstrap_options = "bordered")
```

With `r round(rainfall_stats$pct_zeros, 1)`% of observations being zero, standard linear models (Gaussian) will fail. We need Tweedie, Hurdle or Zero-Inflated models.

# Distributional Analysis

## Rainfall by Day and Month

```{r day-month-dist}
# Day Distribution
day_tab <- df_clean %>%
  filter(rainfall > 0) %>%
  tabyl(day) %>%
  adorn_pct_formatting() %>%
  arrange(desc(n))

print(day_tab)

# Month distribution
month_tab <- df_clean %>%
  filter(rainfall > 0) %>%
  tabyl(month) %>%
  adorn_pct_formatting() %>%
  arrange(desc(n))

print(month_tab)
```

Rainy days are fairly evenly distributed across days of the week, but more concentrated in certain months (June-August, winter months in Australia).

## Cross-tabulation: Month vs Day

```{r cross-tab}
cat("\nCross-tabulation: Month vs Day:\n")
cross_tab <- df_clean %>%
  filter(rainfall > 0) %>%
  tabyl(month, day) %>%
  adorn_totals(c("row", "col"))
print(cross_tab)
```

**Rainfall Frequency by Month and Day**

The cross-tabulation reveals distinct temporal patterns in rainfall occurrence:

1.  Strong Seasonal Seasonality (Row Totals) There is a clear seasonal trend in the frequency of rain events.

**Peak Rainfall Frequency:** The highest number of rainy days occurs in June (5,448), July (5,250), and May (4,937). This corresponds to the Australian winter, indicating that while summer storms might be intense, winter brings a higher frequency of rain events.

**Lowest Rainfall Frequency:** The fewest rainy days occur in February (3,307) and December (3,562), corresponding to the Australian summer.

2.  Uniformity Across Days of the Week (Column Totals)

The distribution of rainfall across days of the week is relatively uniform, ranging from a low of 7,040 (Sunday) to a high of 7,508 (Tuesday).

Physical Interpretation: This is an expected result. Weather systems are driven by atmospheric physics and do not adhere to the human-constructed 7-day work week.

**Data Quality Check:** If we saw a massive spike on specific days (e.g., zero rain on Sundays), it would indicate a data collection error (e.g., staff not measuring gauges on weekends). The consistency here confirms reliable data collection practices.

**Modeling Implication Feature Selection:** The variable Month will be a strong predictor for the "Zero-Inflation" part of the model (predicting if it will rain).

**Noise:** The variable Day (Mon-Sun) likely holds little predictive power and could potentially be excluded or treated as a low-importance feature to prevent overfitting, as the slight variations seen here are likely random noise.

# Correlations with Rainfall

We compute Spearman correlations between rainfall and numeric variables.

```{r correlations}
#| warning: false
numeric_cols <- df_clean %>%
  select(where(is.numeric)) %>%
  names()

numeric_cols <- numeric_cols[numeric_cols != "rainfall"]

cors <- df_clean %>%
  rstatix::cor_test(
    vars = "rainfall",
    vars2 = numeric_cols,
    method = "spearman"
  ) %>%
  filter(!is.na(cor)) %>%
  arrange(desc(abs(cor))) %>%
  dplyr::select(var2, cor, p) %>%
  mutate(
    interpretation = case_when(
      abs(cor) < 0.1 ~ "Negligible",
      abs(cor) < 0.3 ~ "Small",
      abs(cor) < 0.5 ~ "Moderate",
      TRUE ~ "Large"
    )
  )

cors %>%
  kable(caption = "Correlations with rainfall") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


```{r correlation-heatmap}
#| label: fig-corr-heatmap
#| fig-cap: "Spearman Correlation Matrix of Weather Variables"
#| fig-height: 8
#| fig-width: 10
#| warning: false

cor_matrix <- df_clean %>% 
  select(where(is.numeric)) %>% 
  cor(use = "pairwise.complete.obs", method = "spearman")

cor_melt <- cor_matrix %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Var1") %>% 
  pivot_longer(cols = -Var1, names_to = "Var2", values_to = "Correlation")

cor_melt %>% 
  ggplot(aes(x = Var1, y = Var2, fill = Correlation)) + 
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "#D73027", mid = "white", high = "#4575B4", 
    midpoint = 0, limit = c(-1, 1), name = "Spearman\nCorrelation"
  ) +
  theme_minimal() + 
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1), 
    axis.text.y = element_text(size = 10), 
    axis.title = element_blank(), 
    panel.grid.major = element_blank(), 
    legend.position = "right"
  ) +
  geom_text(
    data = filter(cor_melt, abs(Correlation) > 0.3), 
    aes(label = round(Correlation, 2)), 
    color = "black", size = 3
  ) +
  labs(
    title = "Feature Correlation Matrix", 
    subtitle = "Strongest predictors: Humidity (Positive) and Sunshine (Negative)"
  )
```
### Correlation Analysis

We calculated Spearman correlations to identify monotonic relationships between rainfall and atmospheric variables. The results reveal several "Moderate" associations ($|r| > 0.3$) that align with meteorological physics.

**Moisture and Clouds:** The strongest positive correlations are observed with **Humidity** (`r cors$var2[1]` and `r cors$var2[2]`, $r \approx `r round(cors$cor[1], 2)`$) and **Cloud Cover**. This confirms that humidity metrics will be critical features for the logistic component of our zero-inflated model.

**Temperature and Sun:** Significant negative correlations exist with **Sunshine** ($r = `r round(cors$cor[3], 2)`$) and **Maximum Temperature** ($r = `r round(cors$cor[8], 2)`$). This indicates that rainy days in Australia are distinctively cooler and darker than dry days.

**Statistical Significance:** Due to the large sample size, all identified relationships are statistically significant ($p < 0.001$), though variables like `wind_speed` and `min_temp` lack practical predictive power (Interpretation: "`r cors$interpretation[15]`").

# Data Imputation

## Imputation Strategy

Handling missing data in meteorological records requires a nuanced approach, as weather variables exhibit both **temporal autocorrelation** (today's temperature is related to yesterday's) and **multivariate dependency** (humidity affects cloud cover).

Initially, I attempted a simple imputation strategy using global means and medians grouped by location and month. However, diagnostic plots revealed that this approach artificially reduced the variance of the features, leading to "overconfident" models that underestimated the uncertainty in the data.

To correct this, we implemented a **hybrid imputation pipeline**:

1.  **Linear Interpolation (`na.approx`):** For continuous, time-dependent variables like Temperature and Pressure, we used linear interpolation (limited to a 5-day gap). This preserves the natural temporal continuity of weather fronts.
2.  **Chained Random Forests (`missRanger`):** For variables with complex, non-linear dependencies (e.g., Sunshine, Cloud Cover, Evaporation), we used the `missRanger` algorithm. Unlike mean imputation, this method preserves the original distribution and covariance structure of the data by using other features to predict the missing values.

```{r impute-func}
clean_and_impute_weather <- function(df) {
  # Helper function to calculate mode
  calc_mode <- function(x) {
    ux <- unique(na.omit(x))
    if (length(ux) == 0) {
      return(NA)
    }
    ux[which.max(tabulate(match(x, ux)))]
  }

  # Initial clean up
  df <- df %>%
    clean_names() %>%
    mutate(
      date = as.Date(date),
      month = as.factor(month(date)),
      day = as.factor(wday(date, label = TRUE))
    ) %>%
    filter(!is.na(rainfall)) %>%
    select(-rain_tomorrow)

  # Add flags for missing values
  flagged_df <- df %>%
    mutate(
      sunshine_imp_flagged = ifelse(is.na(sunshine), 1, 0),
      evap_imp_flagged = ifelse(is.na(evaporation), 1, 0),
      cloud3pm_imp_flagged = ifelse(is.na(cloud3pm), 1, 0),
      cloud9am_imp_flagged = ifelse(is.na(cloud9am), 1, 0)
    )

  # Interpolate time series like data
  interp_vars <- c(
    "min_temp",
    "max_temp",
    "temp9am",
    "temp3pm",
    "pressure9am",
    "pressure3pm",
    "humidity9am",
    "humidity3pm"
  )

  flagged_df <- flagged_df %>%
    group_by(location) %>%
    mutate(across(
      all_of(interp_vars),
      ~ na.approx(., maxgap = 5, na.rm = FALSE, rule = 2)
    )) %>%
    ungroup()

  metadata_cols <- flagged_df %>%
    select(date)

  imputation_cols <- flagged_df %>%
    select(-date)

  imputed_data <- missRanger(
    imputation_cols,
    pmm.k = 5,
    num.trees = 100,
    sample.fraction = 0.3,
    min.node.size = 10,
    seed = 123,
    verbose = 1,
    maxiter = 5
  )

  imputed_df <- bind_cols(metadata_cols, imputed_data)

  return(imputed_df)
}
```

## Applying Imputation
**Computational Note:** The Random Forest imputation (`missRanger`) is computationally intensive and time-consuming. To ensure the document renders efficiently, this step was executed once and the results were serialized to a CSV file. The code is provided below for reproducibility, but disabled for this session to prioritize rendering speed. We proceed by loading the pre-computed `df_final.csv`.
```{r apply-impute}
#| eval: false
df_final <- clean_and_impute_weather(df)
write_csv(df_final, "data/df_final.csv")
```

```{r load-imputed-data}
#| message: false
df_final <- read_csv("data/df_final.csv")

```
The imputation uses random forest-based methods to fill missing values while preserving relationships.

# Exploratory Data Analysis
## Markov Chain: Yesterday's Rain and Today's Rain

```{r markov-chain}
#| label: fig-markov-chain
#| fig-cap: "Markov Chain Transition Matrix: Probability of rain today conditional on yesterday's weather."
#| fig-height: 6
#| fig-width: 8
#| warning: false

df_clean %>%
  group_by(location) %>%
  arrange(date) %>%
  mutate(yesterday_rain = lag(rain_today)) %>%
  ungroup() %>%
  filter(!is.na(rain_today), !is.na(yesterday_rain)) %>%
  count(yesterday_rain, rain_today) %>%
  group_by(yesterday_rain) %>%
  mutate(prob = n / sum(n)) %>%
  ggplot(aes(x = yesterday_rain, y = rain_today, fill = prob)) +
  geom_tile(color = "white") +
  geom_text(
    aes(label = scales::percent(prob, accuracy = 1)),
    color = "white",
    size = 6,
    fontface = "bold"
  ) +
  scale_fill_viridis_c(option = "D", begin = 0.2, end = 0.8, name = "Probability") +
  labs(
    title = "Do the lags really have an effect?",
    subtitle = "The state of 'Yesterday' strongly predicts the state of 'Today'",
    x = "Did it Rain Yesterday?",
    y = "Did it Rain Today?"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 11),
    plot.title = element_text(face = "bold")
  )
```
This heatmap demonstrates significant **state dependence** (Markov property) in the weather data.

* **Dry Spells Persist:** If it was dry yesterday, there is an **85%** probability it will remain dry today.
* **Rain Persists:** If it rained yesterday, the probability of rain today jumps significantly to **47%** (compared to only 15% if yesterday was dry).

This indicates that lag variables (e.g., `RainYesterday`) will be powerful predictors in our machine learning models.

## Feature Justification: The "Rain Corner"

```{r humidit-sunshine-interaction}
#| label: fig-interaction
#| fig-cap: "2D Density Plot showing the interaction between Sunshine and Humidity on Rainy vs. Dry days."
#| fig-height: 6
#| fig-width: 10
#| warning: false

df_clean %>%
  ggplot(aes(sunshine, humidity3pm)) +
  geom_density2d_filled(bins = 7, show.legend = FALSE) +
  facet_wrap(~rain_today, labeller = label_both) +
  scale_fill_brewer(palette = "Blues") +
  labs(
    title = "Justifying Interaction: Sunshine * Humidity",
    subtitle = "Rain events concentrate in the 'High Humidity / Low Sun' corner",
    x = "Sunshine (hours)",
    y = "Humidity 3pm (%)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    strip.text = element_text(size = 12, face = "bold")
  )
```


Visualizing the joint distribution of **Sunshine** and **Humidity** reveals a distinct structural difference between rainy and dry days.

* **The "Rain Corner" (Right Panel):** On days when it rains (`rain_today: Yes`), the data is tightly clustered in the top-left quadrant, specifically where **Humidity is high (> 75%)** and **Sunshine is low (< 2 hours)**. This confirms that rain isn't just about moisture; it requires the *simultaneous* absence of direct sunlight (cloud cover) and high saturation.
* **The Dry Spread (Left Panel):** Conversely, dry days (`rain_today: No`) show a much broader dispersion, generally trending towards higher sunshine and lower humidity, but without the tight clustering seen in the rain events.

**Modeling Implication:** Because the boundary between rain and no-rain is defined by a specific combination of these two variables rather than a simple linear threshold, we will engineer an **interaction term** (`Sunshine * Humidity`) to help the model capture this non-linear dependency.

## Dry Spell Analysis

```{r dry-spell-plot}
#| label: fig-dry-spells
#| fig-cap: "Empirical probability of rainfall conditioned on the length of the current dry spell."
#| fig-height: 6
#| fig-width: 9
#| warning: false

df_clean %>%
  group_by(location) %>%
  arrange(date) %>%
  mutate(
    # Identify if the previous day was rainy
    did_rain_yesterday = lag(rainfall > 0, default = FALSE),
    # Create a unique ID for each inter-rain period (dry spell)
    dry_spell_id = cumsum(did_rain_yesterday)
  ) %>%
  group_by(location, dry_spell_id) %>%
  # Count how many days into the current spell we are
  mutate(days_since_rain = row_number()) %>%
  ungroup() %>%
  filter(days_since_rain <= 30) %>%
  group_by(days_since_rain) %>%
  # Calculate empirical probability of rain for this specific lag
  summarise(prob_rain = mean(rainfall > 0, na.rm = TRUE)) %>%
  ggplot(aes(x = days_since_rain, y = prob_rain)) +
  geom_line(color = "firebrick", linewidth = 1.2) +
  geom_point(size = 3) +
  geom_smooth(
    method = "loess",
    se = FALSE,
    color = "black",
    linetype = "dashed"
  ) +
  scale_y_continuous(
    labels = scales::percent_format(1),
    breaks = scales::pretty_breaks(n = 6)
  ) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(
    title = "Empirical Probability of Rainfall by Dry Spell Length",
    subtitle = "The probability of rain drops significantly as the dry spell gets longer.",
    x = "Days Since Last Rain",
    y = "Probability of Raining Today"
  ) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  )
```

This analysis highlights the **"memory"** inherent in the weather system, demonstrating that rainfall probability is strictly conditional on recent history.

* **Exponential Decay:** The probability of rain follows a sharp decay curve. If it rained yesterday (Day 1), there is a **60%** chance it will rain today. However, if it has been dry for just 5 days, that probability drops to nearly **20%**.
* **Drought Persistence:** As the dry spell extends beyond two weeks (Day 15+), the probability of rain plateaus at a very low level (< 15%). This suggests that once a high-pressure system (drought) is established, it becomes increasingly difficult to break.

**Modeling Implication:** Because the probability is not constant but depends on the *duration* of the dry state, a simple "Rain Yesterday" binary flag is insufficient. We must engineer a continuous **`days_since_last_rain`** feature to capture this decaying probability.

## Pressure Change Analysis

```{r pressure-plot}
#| label: fig-pressure-plot
#| fig-cap: "Comparison of Atmospheric Pressure at 9am and 3pm for Rainy vs. Dry days."
#| fig-height: 6
#| fig-width: 9
#| warning: false

data_wide <- df_clean %>%
  group_by(rain_today) %>%
  summarise(
    `9:00 AM` = mean(pressure9am, na.rm = TRUE),
    `3:00 PM` = mean(pressure3pm, na.rm = TRUE),
    `Pressure Drop` = mean(pressure9am, na.rm = TRUE) -
      mean(pressure3pm, na.rm = TRUE)
  ) %>%
  pivot_longer(cols = -rain_today, names_to = "metric", values_to = "value") %>%
  mutate(
    metric = factor(metric, levels = c("9:00 AM", "3:00 PM", "Pressure Drop")),
    label_txt = round(value, 1)
  )

ggplot(data_wide, aes(x = rain_today, y = value, fill = rain_today)) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_text(aes(label = label_txt), vjust = -0.5, size = 4, fontface = "bold") +
  facet_wrap(~metric, scales = "free_y") +
  scale_fill_brewer(palette = "Paired") +
  labs(
    title = "Atmospheric Pressure Dynamics",
    subtitle = "Rainy days have lower absolute pressure but a suppressed diurnal drop",
    x = "Rained Today?",
    y = "Pressure (hPa)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(face = "bold")
  )
```

### Feature Engineering: Pressure Differentials

This analysis reveals two distinct physical signals associated with rainfall events:

1.  **Lower Baseline Pressure:** As expected, rainy days are characterized by lower absolute pressure. At 9:00 AM, the average pressure is **1015 hPa** for rainy days compared to **1018.5 hPa** for dry days. This aligns with meteorological theory, where low-pressure systems (cyclones/troughs) facilitate the rising air needed for cloud formation.

2.  **Suppressed Diurnal Variation:** Interestingly, the *change* in pressure throughout the day is different.
    * **Dry Days:** Exhibit a larger pressure drop (**2.7 hPa**) between 9 AM and 3 PM. This is likely driven by strong solar heating (thermal tides) causing air to expand and rise.
    * **Rainy Days:** Show a much smaller drop (**1.3 hPa**). The presence of cloud cover blocks solar heating, suppressing this natural diurnal cycle.

**Modeling Implication:** While `Pressure9am` is a strong predictor on its own, the *difference* between 9am and 3pm (captured by the raw features or an explicit interaction) provides a secondary signal related to cloud cover and solar heating intensity.
